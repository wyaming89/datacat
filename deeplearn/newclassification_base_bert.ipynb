{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "bdf40664-a6ea-42fe-babf-3eab949a578b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-13 15:42:32,544 INFO: Use cuda: True, gpu id: 0.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random, time, os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "root = '/home/admin/jupyter/download/'\n",
    "\n",
    "# set cuda\n",
    "gpu = 0\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "13f38c0d-7b5b-451b-933d-e6a6d2a9d1d6"
   },
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    \"\"\"docstring for Vocab\"\"\"\n",
    "    def __init__(self, text, train=False):\n",
    "        super(Vocab, self).__init__()\n",
    "        self.words = text\n",
    "        self.itos = ['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]']\n",
    "        self.unk_idx = 1\n",
    "        if train:\n",
    "            self.build_vocab(5)\n",
    "\n",
    "\n",
    "    def build_vocab(self, min_freq=1):\n",
    "        from collections import Counter, OrderedDict\n",
    "        from tqdm import tqdm \n",
    "        counter = Counter()\n",
    "        for label,line in tqdm(self.words,total=len(self.words),desc='text loading'):\n",
    "            line = line.split(' ')\n",
    "            counter.update(line)\n",
    "        word_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        word_freq_dict = OrderedDict(word_freq)\n",
    "        for word, freq in word_freq_dict.items():\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "        print('vocab size is %d'%len(self.itos))\n",
    "        self.stoi = {v:i for i,v in enumerate(self.itos)}\n",
    "\n",
    "    def load_vectors(self, file):\n",
    "        with open(file,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        vecmat = np.genformtxt(lines[1:])\n",
    "        words, vec = vecmat[:,0],vecmat[:,1:]\n",
    "        vec_size = vec.shape[1]\n",
    "        vec = np.vstack((np.zeros(2,vec_size),vec))\n",
    "        idx = [words.index(w) for w in self.itos]\n",
    "        self.vector = vec[idx]\n",
    "        \n",
    "    @classmethod\n",
    "    def from_prevocab(cls, vocab_file):\n",
    "        f = open(vocab_file, 'r')\n",
    "        lines = f.readlines()\n",
    "        lines = list(map(lambda x: x.strip(), lines))\n",
    "        cls.stoi = {v:i for i,v in enumerate(lines)}\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "b9061ada-2415-4fc6-b6b7-f91f82f4ce36"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class NLPDataSet(Dataset):\n",
    "    def __init__(self, vocab, data, max_words, max_sent):\n",
    "        super(NLPDataSet,self).__init__()\n",
    "\n",
    "        self.max_sent = max_sent\n",
    "        self.max_words = max_words\n",
    "        self._data = data\n",
    "        self._vocab = vocab\n",
    "        self.content = self.data_deal()\n",
    "\n",
    "    def sentc_clip(self, doc):\n",
    "        doc = doc.split(' ')\n",
    "        index = list(range(0, len(doc), self.max_words-2))\n",
    "        index.append(len(doc))\n",
    "        sentce = []\n",
    "        for i in range(len(index)-1):\n",
    "            line = doc[index[i]:index[i+1]]\n",
    "            line = ['<cls>']+line+['<sep>']\n",
    "            if len(line) < self.max_words:\n",
    "                line = line+['<pad>']*(self.max_words-len(line))\n",
    "            sentce.append(line)\n",
    "        if len(sentce) > self.max_sent:\n",
    "            segment_i = int(self.max_sent/2)\n",
    "            segment_sec = int(segment_i/2)\n",
    "            sentce = sentce[:segment_i]+random.choices(sentce[segment_i:],k=segment_sec)+sentce[-segment_sec:]\n",
    "        if len(sentce) < self.max_sent:\n",
    "            sentce.extend([[0]*self.max_words]*(self.max_sent - len(sentce)))\n",
    "        return sentce\n",
    "        \n",
    "    def data_deal(self):\n",
    "        doc = []\n",
    "        for label,text in tqdm(self._data, total=len(self._data),desc='segment doc'):\n",
    "            res = self.sentc_clip(text)\n",
    "            res = [[self._vocab.stoi.get(s,0) for s in l] for l in res]\n",
    "            doc.append((label, res))\n",
    "        return doc\n",
    "    def __getitem__(self, idx):\n",
    "        label,text = self.content[idx]\n",
    "        return torch.tensor(label), torch.LongTensor(text)\n",
    "    def __len__(self):\n",
    "        return len(self.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "b1d4d9d3-d0e6-4f14-a0e8-cb38598fd8ef"
   },
   "outputs": [],
   "source": [
    "def collate_wrapper(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    doclen=max([len(s) for s in transposed_data[1]])\n",
    "    inp = torch.stack(transposed_data[1], 0)\n",
    "    inp2 = torch.zeros_like(inp)\n",
    "    mask_id = torch.ones_like(inp)\n",
    "    tgt = torch.stack(transposed_data[0], 0)\n",
    "    return tgt,(inp,inp2,mask_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "c59e96f5-69d1-42e3-af7d-a39b72b59881"
   },
   "outputs": [],
   "source": [
    "# build module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b = np.zeros(hidden_size, dtype=np.float32)\n",
    "        self.bias.data.copy_(torch.from_numpy(b))\n",
    "\n",
    "        self.query = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.query.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "    def forward(self, batch_hidden, batch_masks):\n",
    "        # batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)\n",
    "        # batch_masks:  b x len\n",
    "\n",
    "        # linear\n",
    "        key = torch.matmul(batch_hidden, self.weight) + self.bias  # b x len x hidden\n",
    "\n",
    "        # compute attention\n",
    "        outputs = torch.matmul(key, self.query)  # b x len\n",
    "\n",
    "        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n",
    "\n",
    "        attn_scores = F.softmax(masked_outputs, dim=1)  # b x len\n",
    "\n",
    "        # 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0\n",
    "        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n",
    "\n",
    "        # sum weighted sources\n",
    "        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)  # b x hidden\n",
    "\n",
    "        return batch_outputs, attn_scores\n",
    "\n",
    "\n",
    "# build word encoder\n",
    "bert_path = '/home/admin/jupyter/download/emb/'\n",
    "dropout = 0.15\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class WordBertEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WordBertEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#         self.tokenizer = WhitespaceTokenizer()\n",
    "        self.bert = BertModel.from_pretrained(bert_path)\n",
    "\n",
    "        self.pooled = False\n",
    "        logging.info('Build Bert encoder with pooled {}.'.format(self.pooled))\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        pass\n",
    "#         tokens = self.tokenizer.tokenize(tokens)\n",
    "#         return tokens\n",
    "\n",
    "    def get_bert_parameters(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.bert.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        # input_ids: sen_num x bert_len\n",
    "        # token_type_ids: sen_num  x bert_len\n",
    "\n",
    "        # sen_num x bert_len x 256, sen_num x 256\n",
    "        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        if self.pooled:\n",
    "            reps = pooled_output\n",
    "        else:\n",
    "            reps = sequence_output[:, 0, :]  # sen_num x 256\n",
    "\n",
    "        if self.training:\n",
    "            reps = self.dropout(reps)\n",
    "\n",
    "        return reps\n",
    "\n",
    "\n",
    "class WhitespaceTokenizer():\n",
    "    \"\"\"WhitespaceTokenizer with vocab.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        vocab_file = bert_path + 'vocab.txt'\n",
    "        self._token2id = self.load_vocab(vocab_file)\n",
    "        self._id2token = {v: k for k, v in self._token2id.items()}\n",
    "        self.max_len = 256\n",
    "        self.unk = 1\n",
    "\n",
    "        logging.info(\"Build Bert vocab with size %d.\" % (self.vocab_size))\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        f = open(vocab_file, 'r')\n",
    "        lines = f.readlines()\n",
    "        lines = list(map(lambda x: x.strip(), lines))\n",
    "        vocab = dict(zip(lines, range(len(lines))))\n",
    "        return vocab\n",
    "\n",
    "    def tokenize(self, tokens):\n",
    "        assert len(tokens) <= self.max_len - 2\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        output_tokens = self.token2id(tokens)\n",
    "        return output_tokens\n",
    "\n",
    "    def token2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._token2id.get(x, self.unk) for x in xs]\n",
    "        return self._token2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._id2token)\n",
    "\n",
    "\n",
    "# build sent encoder\n",
    "sent_hidden_size = 256\n",
    "sent_num_layers = 2\n",
    "\n",
    "\n",
    "class SentEncoder(nn.Module):\n",
    "    def __init__(self, sent_rep_size):\n",
    "        super(SentEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sent_lstm = nn.LSTM(\n",
    "            input_size=sent_rep_size,\n",
    "            hidden_size=sent_hidden_size,\n",
    "            num_layers=sent_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, sent_reps, sent_masks):\n",
    "        # sent_reps:  b x doc_len x sent_rep_size\n",
    "        # sent_masks: b x doc_len\n",
    "\n",
    "        sent_hiddens, _ = self.sent_lstm(sent_reps)  # b x doc_len x hidden*2\n",
    "        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n",
    "\n",
    "        if self.training:\n",
    "            sent_hiddens = self.dropout(sent_hiddens)\n",
    "\n",
    "        return sent_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "03e6d742-8069-423a-b040-3a1af46d874f"
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, cls_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.sent_rep_size = 256\n",
    "        self.doc_rep_size = sent_hidden_size * 2\n",
    "        self.all_parameters = {}\n",
    "        parameters = []\n",
    "        self.word_encoder = WordBertEncoder()\n",
    "        bert_parameters = self.word_encoder.get_bert_parameters()\n",
    "\n",
    "        self.sent_encoder = SentEncoder(self.sent_rep_size)\n",
    "        self.sent_attention = Attention(self.doc_rep_size)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n",
    "\n",
    "        self.out = nn.Linear(self.doc_rep_size, cls_size, bias=True)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n",
    "\n",
    "        if use_cuda:\n",
    "            self.to(device)\n",
    "\n",
    "        if len(parameters) > 0:\n",
    "            self.all_parameters[\"basic_parameters\"] = parameters\n",
    "        self.all_parameters[\"bert_parameters\"] = bert_parameters\n",
    "\n",
    "        logging.info('Build model with bert word encoder, lstm sent encoder.')\n",
    "\n",
    "        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n",
    "        logging.info('Model param num: %.2f M.' % (para_num / 1e6))\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        # batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len\n",
    "        # batch_masks : b x doc_len x sent_len\n",
    "        batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n",
    "        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n",
    "        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "\n",
    "        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)  # sen_num x sent_rep_size\n",
    "\n",
    "        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  # b x doc_len x sent_rep_size\n",
    "        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  # b x doc_len x max_sent_len\n",
    "        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n",
    "\n",
    "        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  # b x doc_len x doc_rep_size\n",
    "        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  # b x doc_rep_size\n",
    "\n",
    "        batch_outputs = self.out(doc_reps)  # b x num_labels\n",
    "\n",
    "        return batch_outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "5c73868f-f112-4a73-a10c-efe482f6b81e"
   },
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "learning_rate = 2e-4\n",
    "bert_lr = 5e-5\n",
    "decay = .75\n",
    "decay_step = 1000\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, model_parameters, steps):\n",
    "        self.all_params = []\n",
    "        self.optims = []\n",
    "        self.schedulers = []\n",
    "\n",
    "        for name, parameters in model_parameters.items():\n",
    "            if name.startswith(\"basic\"):\n",
    "                optim = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "                self.optims.append(optim)\n",
    "\n",
    "                l = lambda step: decay ** (step // decay_step)\n",
    "                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n",
    "                self.schedulers.append(scheduler)\n",
    "                self.all_params.extend(parameters)\n",
    "            elif name.startswith(\"bert\"):\n",
    "                optim_bert = AdamW(parameters, bert_lr, eps=1e-8)\n",
    "                self.optims.append(optim_bert)\n",
    "\n",
    "                scheduler_bert = get_linear_schedule_with_warmup(optim_bert, 0, steps)\n",
    "                self.schedulers.append(scheduler_bert)\n",
    "\n",
    "                for group in parameters:\n",
    "                    for p in group['params']:\n",
    "                        self.all_params.append(p)\n",
    "            else:\n",
    "                Exception(\"no nameed parameters.\")\n",
    "\n",
    "        self.num = len(self.optims)\n",
    "\n",
    "    def step(self):\n",
    "        for optim, scheduler in zip(self.optims, self.schedulers):\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for optim in self.optims:\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def get_lr(self):\n",
    "        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n",
    "        lr = ' %.5f' * self.num\n",
    "        res = lr % lrs\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "uuid": "2388650d-cc9a-4a11-9b8a-85d581bceb34"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_score(y_ture, y_pred):\n",
    "    y_ture = np.array(y_ture)\n",
    "    y_pred = np.array(y_pred)\n",
    "    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n",
    "    p = precision_score(y_ture, y_pred, average='macro') * 100\n",
    "    r = recall_score(y_ture, y_pred, average='macro') * 100\n",
    "\n",
    "    return str((reformat(p, 2), reformat(r, 2), reformat(f1, 2))), reformat(f1, 2)\n",
    "\n",
    "\n",
    "def reformat(num, n):\n",
    "    return float(format(num, '0.' + str(n) + 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "uuid": "9f2e6ef7-b90f-466d-a605-ee2d2c82cde9"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch_num, data_loader, criterion, optimizer, device):\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    total_num = len(data_loader)\n",
    "    losses = 0\n",
    "    overall_losses = 0\n",
    "    for epoch in range(1,epoch_num+1):\n",
    "        for idx, (label, inp) in tqdm(enumerate(data_loader),total=total_num, desc='training'):\n",
    "            torch.cuda.empty_cache()\n",
    "            inp = (inp[0].to(device),inp[1].to(device),inp[2].to(device))\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = model(inp)\n",
    "            loss = criterion(output,label)\n",
    "            loss_value = loss.detach().cpu().item()\n",
    "            loss.backward()\n",
    "            losses += loss_value\n",
    "            overall_losses += loss_value\n",
    "            y_pred.extend(torch.max(output, dim=1)[1].cpu().numpy().tolist())\n",
    "            y_true.extend(label.cpu().numpy().tolist())\n",
    "            nn.utils.clip_grad_norm_(optimizer.all_params,5.0)\n",
    "            for optim, scheduler in zip(optimizer.optims, optimizer.schedulers):\n",
    "                optim.step()\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step += 1\n",
    "            if (idx+1) %500 ==0:\n",
    "                elapsed = time.time() - start_time\n",
    "                lrs = optimizer.get_lr()\n",
    "                print(\n",
    "                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n",
    "                        epoch, step, idx, total_num, lrs,\n",
    "                        losses / 50,\n",
    "                        elapsed / 50))\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n",
    "                        epoch, step, idx, total_num, lrs,\n",
    "                        losses / 50,\n",
    "                        elapsed / 50))\n",
    "\n",
    "                losses = 0\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    total_num = len(data_loader)\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for idx, (label, inp) in tqdm(enumerate(data_loader), total=total_num ):\n",
    "        torch.cuda.empty_cache()\n",
    "        inp = (inp[0].to(device),inp[1].to(device),inp[2].to(device))\n",
    "        with torch.no_grad():\n",
    "            output = model(inp)\n",
    "            pred = output.argmax(1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(label.tolist())\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    torch.save(model.state_dict(),'./bert.pth')\n",
    "    print(report)\n",
    "  \n",
    "\n",
    "def pred(model, data_loader, device):\n",
    "    save_test = 'bert.csv'\n",
    "#     model = Model(14)\n",
    "#     if use_cuda:\n",
    "#         model.load_state_dict(torch.load(model_state))\n",
    "#     else:\n",
    "#         model.load_state_dict(torch.load(model_state,map_location='cpu'))\n",
    "    total_num = len(data_loader)\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    for label,inp in tqdm(data_loader, total=total_num, desc='predicting'):\n",
    "        inp = (inp[0].to(device),inp[1].to(device),inp[2].to(device))\n",
    "        with torch.no_grad():\n",
    "            output = model(inp)\n",
    "            pred = output.argmax(1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "    df = pd.DataFrame({'label': y_pred})\n",
    "    df.to_csv(save_test, index=False, sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "2a0eee4f-d98a-4dcd-a69d-d37754856cca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "uuid": "ca0054b5-235b-4c85-95f7-d22ace6b30f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-13 15:43:00,294 INFO: Build Bert encoder with pooled False.\n",
      "2021-01-13 15:43:17,119 INFO: Build model with bert word encoder, lstm sent encoder.\n",
      "2021-01-13 15:43:17,125 INFO: Model param num: 7.72 M.\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "save_model = os.path.join(root,'bert.pth')\n",
    "model = Model(14)\n",
    "# model.load_state_dict(torch.load(save_model))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Optimizer(model.all_parameters, steps=batch_size * epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "uuid": "0ba5c088-34cb-4b93-95b5-f14df9330619"
   },
   "outputs": [],
   "source": [
    "vocab_file = './emb/vocab.txt'\n",
    "vocab = Vocab.from_prevocab(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "uuid": "250f3c2f-ca74-41bc-ba3b-ca9f69580efd"
   },
   "outputs": [],
   "source": [
    "\n",
    "fold_num = 5\n",
    "data_file = os.path.join(root,'data/train_set.csv')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "uuid": "46b69a02-a165-45a8-8f9c-aa96e6f6ef84"
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    test_file = './data/test_a.csv'\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    test_df['label'] = 0\n",
    "    test_df = test_df.loc[:,['label','text']]\n",
    "    test_set = NLPDataSet(vocab, test_df.values, 256, 8)\n",
    "    test_loader = DataLoader(test_set, 1, collate_fn=collate_wrapper)\n",
    "    pred(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "uuid": "0c9b6a52-7a5d-4a2e-ad44-c144f96f6a84"
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "    df = pd.read_csv(data_file, sep='\\t',encoding='utf-8')\n",
    "    kf = StratifiedKFold(fold_num, shuffle=False)\n",
    "    kf_index = kf.split(df['text'],df['label'])\n",
    "    for train_idx, val_idx in kf_index:\n",
    "        train_data = df.iloc[train_idx].values\n",
    "        train_set = NLPDataSet(vocab, train_data, 256, 8)\n",
    "        train_loader = DataLoader(train_set, batch_size, collate_fn=collate_wrapper)\n",
    "        train(model, epochs, train_loader, criterion, optimizer,device)\n",
    "\n",
    "        val_data = df.iloc[val_idx].values\n",
    "        val_set = NLPDataSet(vocab, val_data, 256, 8)\n",
    "        val_loader = DataLoader(val_set, batch_size, collate_fn=collate_wrapper)\n",
    "        test(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "74853d79-2d2e-407e-ae50-af9e059440a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segment doc: 100%|██████████| 160000/160000 [01:35<00:00, 1667.42it/s]\n",
      "training:  10%|▉         | 499/5000 [06:00<54:09,  1.39it/s]2021-01-13 15:51:02,300 INFO: | epoch   1 | step 500 | batch 499/5000 | lr 0.00020 0.00000 | loss 11.1667 | s/batch 7.23\n",
      "training:  10%|█         | 500/5000 [06:01<54:09,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 500 | batch 499/5000 | lr 0.00020 0.00000 | loss 11.1667 | s/batch 7.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|█▉        | 999/5000 [12:02<48:11,  1.38it/s]2021-01-13 15:57:03,855 INFO: | epoch   1 | step 1000 | batch 999/5000 | lr 0.00015 0.00000 | loss 7.2187 | s/batch 14.46\n",
      "training:  20%|██        | 1000/5000 [12:03<48:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 1000 | batch 999/5000 | lr 0.00015 0.00000 | loss 7.2187 | s/batch 14.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|██▉       | 1499/5000 [18:06<42:22,  1.38it/s]2021-01-13 16:03:08,024 INFO: | epoch   1 | step 1500 | batch 1499/5000 | lr 0.00015 0.00000 | loss 6.4490 | s/batch 21.75\n",
      "training:  30%|███       | 1500/5000 [18:07<42:19,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 1500 | batch 1499/5000 | lr 0.00015 0.00000 | loss 6.4490 | s/batch 21.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|███▉      | 1999/5000 [24:07<36:12,  1.38it/s]2021-01-13 16:09:09,327 INFO: | epoch   1 | step 2000 | batch 1999/5000 | lr 0.00011 0.00000 | loss 6.3312 | s/batch 28.97\n",
      "training:  40%|████      | 2000/5000 [24:08<36:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 2000 | batch 1999/5000 | lr 0.00011 0.00000 | loss 6.3312 | s/batch 28.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|████▉     | 2499/5000 [30:09<30:25,  1.37it/s]2021-01-13 16:15:10,776 INFO: | epoch   1 | step 2500 | batch 2499/5000 | lr 0.00011 0.00000 | loss 5.9428 | s/batch 36.20\n",
      "training:  50%|█████     | 2500/5000 [30:10<30:27,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 2500 | batch 2499/5000 | lr 0.00011 0.00000 | loss 5.9428 | s/batch 36.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|█████▉    | 2999/5000 [36:10<24:05,  1.38it/s]2021-01-13 16:21:12,411 INFO: | epoch   1 | step 3000 | batch 2999/5000 | lr 0.00008 0.00000 | loss 5.9615 | s/batch 43.43\n",
      "training:  60%|██████    | 3000/5000 [36:11<24:04,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 3000 | batch 2999/5000 | lr 0.00008 0.00000 | loss 5.9615 | s/batch 43.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|██████▉   | 3499/5000 [42:12<18:04,  1.38it/s]2021-01-13 16:27:13,668 INFO: | epoch   1 | step 3500 | batch 3499/5000 | lr 0.00008 0.00000 | loss 5.7850 | s/batch 50.66\n",
      "training:  70%|███████   | 3500/5000 [42:12<18:04,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 3500 | batch 3499/5000 | lr 0.00008 0.00000 | loss 5.7850 | s/batch 50.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|███████▉  | 3999/5000 [48:13<12:04,  1.38it/s]2021-01-13 16:33:14,869 INFO: | epoch   1 | step 4000 | batch 3999/5000 | lr 0.00006 0.00000 | loss 5.6777 | s/batch 57.88\n",
      "training:  80%|████████  | 4000/5000 [48:14<12:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 4000 | batch 3999/5000 | lr 0.00006 0.00000 | loss 5.6777 | s/batch 57.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|████████▉ | 4499/5000 [54:14<06:00,  1.39it/s]2021-01-13 16:39:15,976 INFO: | epoch   1 | step 4500 | batch 4499/5000 | lr 0.00006 0.00000 | loss 5.4930 | s/batch 65.10\n",
      "training:  90%|█████████ | 4500/5000 [54:15<06:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 4500 | batch 4499/5000 | lr 0.00006 0.00000 | loss 5.4930 | s/batch 65.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|█████████▉| 4999/5000 [1:00:15<00:00,  1.39it/s]2021-01-13 16:45:17,198 INFO: | epoch   1 | step 5000 | batch 4999/5000 | lr 0.00005 0.00000 | loss 5.4191 | s/batch 72.33\n",
      "training: 100%|██████████| 5000/5000 [1:00:16<00:00,  1.38it/s]\n",
      "segment doc:   0%|          | 187/40000 [00:00<00:21, 1868.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 5000 | batch 4999/5000 | lr 0.00005 0.00000 | loss 5.4191 | s/batch 72.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segment doc: 100%|██████████| 40000/40000 [00:22<00:00, 1809.19it/s]\n",
      "100%|██████████| 1250/1250 [04:20<00:00,  4.80it/s]\n",
      "segment doc:   0%|          | 189/160000 [00:00<01:24, 1887.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.84      7784\n",
      "           1       0.78      0.91      0.84      7389\n",
      "           2       0.95      0.93      0.94      6285\n",
      "           3       0.79      0.89      0.84      4426\n",
      "           4       0.82      0.72      0.77      3003\n",
      "           5       0.80      0.81      0.80      2446\n",
      "           6       0.90      0.80      0.84      1997\n",
      "           7       0.83      0.61      0.70      1768\n",
      "           8       0.55      0.72      0.63      1570\n",
      "           9       0.84      0.70      0.76      1176\n",
      "          10       0.79      0.83      0.81       984\n",
      "          11       0.78      0.61      0.69       627\n",
      "          12       0.92      0.69      0.79       364\n",
      "          13       0.59      0.42      0.49       181\n",
      "\n",
      "    accuracy                           0.83     40000\n",
      "   macro avg       0.80      0.75      0.77     40000\n",
      "weighted avg       0.83      0.83      0.83     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segment doc: 100%|██████████| 160000/160000 [01:40<00:00, 1587.50it/s]\n",
      "training:  10%|▉         | 499/5000 [06:00<54:07,  1.39it/s]2021-01-13 16:57:43,053 INFO: | epoch   1 | step 500 | batch 499/5000 | lr 0.00005 0.00000 | loss 5.4862 | s/batch 7.22\n",
      "training:  10%|█         | 500/5000 [06:01<54:08,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 500 | batch 499/5000 | lr 0.00005 0.00000 | loss 5.4862 | s/batch 7.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|█▉        | 999/5000 [12:01<48:10,  1.38it/s]2021-01-13 17:03:44,424 INFO: | epoch   1 | step 1000 | batch 999/5000 | lr 0.00004 0.00000 | loss 5.3011 | s/batch 14.45\n",
      "training:  20%|██        | 1000/5000 [12:02<48:12,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 1000 | batch 999/5000 | lr 0.00004 0.00000 | loss 5.3011 | s/batch 14.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|██▉       | 1499/5000 [18:03<42:11,  1.38it/s]2021-01-13 17:09:45,810 INFO: | epoch   1 | step 1500 | batch 1499/5000 | lr 0.00004 0.00000 | loss 5.3251 | s/batch 21.68\n",
      "training:  30%|███       | 1500/5000 [18:03<42:10,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 1500 | batch 1499/5000 | lr 0.00004 0.00000 | loss 5.3251 | s/batch 21.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|███▉      | 1999/5000 [24:04<36:09,  1.38it/s]2021-01-13 17:15:47,179 INFO: | epoch   1 | step 2000 | batch 1999/5000 | lr 0.00003 0.00000 | loss 5.2256 | s/batch 28.91\n",
      "training:  40%|████      | 2000/5000 [24:05<36:10,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 2000 | batch 1999/5000 | lr 0.00003 0.00000 | loss 5.2256 | s/batch 28.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|████▉     | 2499/5000 [30:06<30:08,  1.38it/s]2021-01-13 17:21:48,770 INFO: | epoch   1 | step 2500 | batch 2499/5000 | lr 0.00003 0.00000 | loss 5.0434 | s/batch 36.14\n",
      "training:  50%|█████     | 2500/5000 [30:06<30:08,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 2500 | batch 2499/5000 | lr 0.00003 0.00000 | loss 5.0434 | s/batch 36.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  60%|█████▉    | 2999/5000 [36:07<24:03,  1.39it/s]2021-01-13 17:27:50,099 INFO: | epoch   1 | step 3000 | batch 2999/5000 | lr 0.00002 0.00000 | loss 5.1398 | s/batch 43.36\n",
      "training:  60%|██████    | 3000/5000 [36:08<24:04,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 3000 | batch 2999/5000 | lr 0.00002 0.00000 | loss 5.1398 | s/batch 43.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|██████▉   | 3499/5000 [42:08<18:03,  1.38it/s]2021-01-13 17:33:51,287 INFO: | epoch   1 | step 3500 | batch 3499/5000 | lr 0.00002 0.00000 | loss 5.1040 | s/batch 50.59\n",
      "training:  70%|███████   | 3500/5000 [42:09<18:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 3500 | batch 3499/5000 | lr 0.00002 0.00000 | loss 5.1040 | s/batch 50.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  80%|███████▉  | 3999/5000 [48:10<12:03,  1.38it/s]2021-01-13 17:39:53,008 INFO: | epoch   1 | step 4000 | batch 3999/5000 | lr 0.00002 0.00000 | loss 5.1010 | s/batch 57.82\n",
      "training:  80%|████████  | 4000/5000 [48:11<12:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 4000 | batch 3999/5000 | lr 0.00002 0.00000 | loss 5.1010 | s/batch 57.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  90%|████████▉ | 4499/5000 [54:12<06:02,  1.38it/s]2021-01-13 17:45:54,813 INFO: | epoch   1 | step 4500 | batch 4499/5000 | lr 0.00002 0.00000 | loss 5.0051 | s/batch 65.06\n",
      "training:  90%|█████████ | 4500/5000 [54:12<06:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 4500 | batch 4499/5000 | lr 0.00002 0.00000 | loss 5.0051 | s/batch 65.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|█████████▉| 4999/5000 [1:00:14<00:00,  1.38it/s]2021-01-13 17:51:56,714 INFO: | epoch   1 | step 5000 | batch 4999/5000 | lr 0.00001 0.00000 | loss 4.9691 | s/batch 72.30\n",
      "training: 100%|██████████| 5000/5000 [1:00:14<00:00,  1.38it/s]\n",
      "segment doc:   0%|          | 188/40000 [00:00<00:21, 1872.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 5000 | batch 4999/5000 | lr 0.00001 0.00000 | loss 4.9691 | s/batch 72.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segment doc: 100%|██████████| 40000/40000 [00:22<00:00, 1802.93it/s]\n",
      "100%|██████████| 1250/1250 [04:21<00:00,  4.78it/s]\n",
      "segment doc:   0%|          | 186/160000 [00:00<01:26, 1854.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      7783\n",
      "           1       0.81      0.90      0.85      7389\n",
      "           2       0.95      0.94      0.95      6285\n",
      "           3       0.86      0.87      0.86      4427\n",
      "           4       0.79      0.76      0.78      3003\n",
      "           5       0.79      0.83      0.81      2446\n",
      "           6       0.88      0.83      0.86      1997\n",
      "           7       0.83      0.66      0.73      1769\n",
      "           8       0.62      0.73      0.67      1570\n",
      "           9       0.83      0.73      0.77      1176\n",
      "          10       0.81      0.82      0.81       984\n",
      "          11       0.78      0.69      0.73       626\n",
      "          12       0.88      0.72      0.79       364\n",
      "          13       0.73      0.52      0.61       181\n",
      "\n",
      "    accuracy                           0.84     40000\n",
      "   macro avg       0.82      0.77      0.79     40000\n",
      "weighted avg       0.84      0.84      0.84     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "segment doc: 100%|██████████| 160000/160000 [01:37<00:00, 1644.79it/s]\n",
      "training:  10%|▉         | 499/5000 [06:00<54:00,  1.39it/s]2021-01-13 18:04:20,173 INFO: | epoch   1 | step 500 | batch 499/5000 | lr 0.00001 0.00000 | loss 5.1452 | s/batch 7.22\n",
      "training:  10%|█         | 500/5000 [06:01<54:05,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 500 | batch 499/5000 | lr 0.00001 0.00000 | loss 5.1452 | s/batch 7.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  20%|█▉        | 999/5000 [12:01<48:04,  1.39it/s]2021-01-13 18:10:21,219 INFO: | epoch   1 | step 1000 | batch 999/5000 | lr 0.00001 0.00000 | loss 4.9712 | s/batch 14.44\n",
      "training:  20%|██        | 1000/5000 [12:02<48:06,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 1000 | batch 999/5000 | lr 0.00001 0.00000 | loss 4.9712 | s/batch 14.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  30%|██▉       | 1499/5000 [18:02<42:08,  1.38it/s]2021-01-13 18:16:22,312 INFO: | epoch   1 | step 1500 | batch 1499/5000 | lr 0.00001 0.00000 | loss 4.9959 | s/batch 21.66\n",
      "training:  30%|███       | 1500/5000 [18:03<42:09,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 1500 | batch 1499/5000 | lr 0.00001 0.00000 | loss 4.9959 | s/batch 21.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  40%|███▉      | 1999/5000 [24:03<36:07,  1.38it/s]2021-01-13 18:22:23,720 INFO: | epoch   1 | step 2000 | batch 1999/5000 | lr 0.00001 0.00000 | loss 5.0172 | s/batch 28.89\n",
      "training:  40%|████      | 2000/5000 [24:04<36:08,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 2000 | batch 1999/5000 | lr 0.00001 0.00000 | loss 5.0172 | s/batch 28.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|████▉     | 2499/5000 [30:05<30:07,  1.38it/s]2021-01-13 18:28:24,875 INFO: | epoch   1 | step 2500 | batch 2499/5000 | lr 0.00001 0.00000 | loss 4.9621 | s/batch 36.11\n",
      "training:  50%|█████     | 2500/5000 [30:05<30:07,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | step 2500 | batch 2499/5000 | lr 0.00001 0.00000 | loss 4.9621 | s/batch 36.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  50%|█████     | 2518/5000 [30:18<29:53,  1.38it/s]"
     ]
    }
   ],
   "source": [
    "training()\n",
    "# predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "136375",
   "source": "dsw"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
